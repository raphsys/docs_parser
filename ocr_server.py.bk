import os
import time
import torch
import uvicorn
import fitz  # PyMuPDF
import shutil
import subprocess
from PIL import Image, ImageDraw, ImageEnhance
from fastapi import FastAPI, UploadFile, File
from fastapi.responses import JSONResponse
from fastapi.staticfiles import StaticFiles
from transformers import AutoProcessor, Qwen2VLForConditionalGeneration
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
import warnings

warnings.filterwarnings("ignore", category=UserWarning)

# ─── Configuration performance CPU ───
os.environ["OMP_NUM_THREADS"] = "4"
os.environ["MKL_NUM_THREADS"] = "4"
torch.set_num_threads(4)
torch.set_num_interop_threads(1)

device = "cpu"
# Utilisation de bfloat16 pour diviser par 2 la consommation RAM (4.5 Go au lieu de 9 Go)
# C'est beaucoup plus stable que la quantization dynamique pour les petites RAM
dtype = torch.bfloat16 

print(f"Using device: {device}, dtype: {dtype}")

# ─── Chargement modèle Qwen2-VL-2B (LOCAL) ───
MODEL_ID = "./ai_models/qwen2vl"

print(f">>> Chargement de {MODEL_ID} (Mode Basse Consommation RAM) ...")

try:
    # Patch technique obligatoire pour éviter les erreurs d'attribut
    import transformers.modeling_utils
    transformers.modeling_utils.PreTrainedModel._supports_sdpa = False

    processor = AutoProcessor.from_pretrained(
        MODEL_ID, 
        trust_remote_code=True,
        min_pixels=256*28*28,
        max_pixels=768*28*28 # Limite la taille des tensors d'image en RAM
    )

    # Chargement direct en bfloat16
    model = Qwen2VLForConditionalGeneration.from_pretrained(
        MODEL_ID,
        torch_dtype=dtype,
        device_map="cpu", 
        low_cpu_mem_usage=True,
        trust_remote_code=True
    ).eval()

    print(">>> [SUCCÈS] Modèle Qwen2-VL chargé en BFloat16. <<<")
except Exception as e:
    print(f"!!! ERREUR critique chargement : {e}")
    model = None

# ─── Constantes & Dirs ───
UPLOAD_DIR, CONV_DIR, RESULTS_DIR = 'uploads', 'converted_pages', 'ocr_results'
for d in [UPLOAD_DIR, CONV_DIR, RESULTS_DIR]:
    if not os.path.exists(d): os.makedirs(d)

app = FastAPI(title="IA Document WYSIWYG")
app.mount("/results", StaticFiles(directory=RESULTS_DIR), name="results")

def draw_bboxes(pil_img, content, output_path):
    draw = ImageDraw.Draw(pil_img)
    colors = {'[TABLE]': 'green', '[TEXT]': 'blue', '[AI_TEXT]': 'red'}
    bbox_pattern = re.compile(r'bbox=\((\d+),\s*(\d+),\s*(\d+),\s*(\d+)\)')
    for line in content.split('\n'):
        match = bbox_pattern.search(line)
        if match:
            try:
                x0, y0, x1, y1 = map(int, match.groups())
                draw.rectangle([x0, y0, x1, y1], outline='red', width=3)
            except: continue
    pil_img.save(output_path)

def reconstruct_legacy_content(structure):
    lines = []
    dim, lay = structure.get("dimensions", {}), structure.get("layout", {})
    m = lay.get("margins", {"top":0, "bottom":0, "left":0, "right":0})
    lines.append(f'[PAGE_INFO] num={structure.get("page_number")} size={dim.get("width")}x{dim.get("height")} rot={lay.get("rotation")} margins={m}')
    for table in structure.get("tables", []):
        b = table["bbox"]
        lines.append(f'[TABLE]\n{table.get("markdown")}\nbbox=({int(b[0])},{int(b[1])},{int(b[2])},{int(b[3])})')
    for block in structure.get("blocks", []):
        if block.get("in_table") or block["type"] != "text": continue
        tag = "AI_TEXT" if block.get("source") == "ai" else "TEXT"
        for line in block.get("lines", []):
            txt = "".join([s["text"] for s in line["spans"]]).strip()
            if not txt: continue
            lb = line["bbox"]
            lines.append(f'[{tag}] "{txt}" bbox=({int(lb[0])},{int(lb[1])},{int(lb[2])},{int(lb[3])})')
    return "\n".join(lines)

def run_pro_ocr(pil_image: Image.Image):
    if not model: return []
    w, h = pil_image.size
    
    task_prompt = "Read the text in the image and provide bounding boxes for each line in the format: [text] (y1,x1,y2,x2)"
    messages = [{"role": "user", "content": [{"type": "image"}, {"type": "text", "text": task_prompt}]}]
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    try:
        inputs = processor(text=[text], images=[pil_image], return_tensors="pt").to(device)
        # S'assurer que les inputs sont aussi en bfloat16 pour le CPU
        if "pixel_values" in inputs:
            inputs["pixel_values"] = inputs["pixel_values"].to(dtype)
            
        with torch.inference_mode():
            generated_ids = model.generate(**inputs, max_new_tokens=1024, do_sample=False)
        
        gen_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        answer = gen_text.split("assistant\n")[-1]
        
        results = []
        pattern = re.compile(r'\[(.*?)\]\s*\((\d+),(\d+),(\d+),(\d+)\)')
        for match in pattern.finditer(answer):
            txt, y1, x1, y2, x2 = match.groups()
            bbox = [
                int(int(x1) * w / 1000), int(int(y1) * h / 1000),
                int(int(x2) * w / 1000), int(int(y2) * h / 1000)
            ]
            results.append({"label": txt.strip(), "bbox": bbox})
        return results
    except Exception as e:
        print(f"Erreur OCR Qwen : {e}")
        return []

def process_single_page(args):
    i, page_data, force_ai, filename = args
    start_time = time.time()
    pix = page_data.get_pixmap(dpi=150)
    img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
    
    from structure_extractor import DocumentParser
    parser = DocumentParser()
    structure = parser.parse_page(page_data, i, scale_x=pix.width/page_data.rect.width, scale_y=pix.height/page_data.rect.height)

    native_text_len = sum(len(s["text"]) for b in structure["blocks"] if b["type"]=="text" for l in b["lines"] for s in l["spans"])
    
    if force_ai or native_text_len < 400:
        print(f"  > IA Inférence pour page {i+1}...")
        ai_data = run_pro_ocr(img)
        from structure_extractor import VisualAttributeExtractor
        ve = VisualAttributeExtractor()
        native_text_full = " ".join([s["text"] for b in structure["blocks"] if b["type"]=="text" for l in b["lines"] for s in l["spans"]]).lower()

        for item in ai_data:
            if item["label"].lower() not in native_text_full:
                style = ve.analyze(img, item["bbox"])
                structure["blocks"].append({
                    "type": "text", "bbox": item["bbox"], "source": "ai", "role": "paragraph",
                    "lines": [{"bbox": item["bbox"], "spans": [{"text": item["label"], "style": style, "bbox": item["bbox"]}]}]
                })

    final_content = reconstruct_legacy_content(structure)
    vis_fn = f"vis_{filename}_p{i+1}.jpg"
    draw_bboxes(img.copy(), final_content, os.path.join(RESULTS_DIR, vis_fn))
    print(f"Page {i+1} traitée en {time.time() - start_time:.2f}s")
    return {"page": i+1, "content": final_content, "structure": structure, "visual_url": f"/results/{vis_fn}", "status": "success"}

@app.post("/ocr")
async def perform_ocr(file: UploadFile = File(...), force_ai: bool = False):
    save_path = os.path.join(UPLOAD_DIR, file.filename)
    with open(save_path, "wb") as buffer: shutil.copyfileobj(file.file, buffer)
    ext = os.path.splitext(file.filename)[1].lower()

    try:
        if ext in ['.docx', '.doc', '.pptx', '.ppt']:
            subprocess.run(['libreoffice', '--headless', '--convert-to', 'pdf', '--outdir', CONV_DIR, save_path], check=True)
            save_path = os.path.join(CONV_DIR, os.path.splitext(file.filename)[0] + ".pdf")
            ext = '.pdf'

        if ext == '.pdf':
            doc = fitz.open(save_path)
            results = [process_single_page((i, doc[i], force_ai, file.filename)) for i in range(len(doc))]
            return JSONResponse(content={"results": results, "status": "success"})
        else:
            img = Image.open(save_path).convert("RGB")
            ai_data = run_pro_ocr(img)
            from structure_extractor import VisualAttributeExtractor
            ve = VisualAttributeExtractor()
            blocks = []
            for item in ai_data:
                style = ve.analyze(img, item["bbox"])
                blocks.append({"type": "text", "bbox": item["bbox"], "source": "ai", "lines": [{"bbox": item["bbox"], "spans": [{"text": item["label"], "style": style, "bbox": item["bbox"]}]}]})
            structure = {"page_number": 1, "dimensions": {"width": img.width, "height": img.height}, "blocks": blocks}
            content = reconstruct_legacy_content(structure)
            vis_fn = f"vis_{file.filename}.jpg"
            draw_bboxes(img.copy(), content, os.path.join(RESULTS_DIR, vis_fn))
            return JSONResponse(content={"results": [{"page": 1, "content": content, "structure": structure, "visual_url": f"/results/{vis_fn}"}], "status": "success"})
    except Exception as e:
        return JSONResponse(content={"error": str(e)}, status_code=500)

@app.post("/reconstruct")
async def reconstruct_document(data: dict):
    try:
        from reconstructor import DocumentReconstructor
        recon = DocumentReconstructor()
        output_path = os.path.join(RESULTS_DIR, "reconstructed_output.pdf")
        recon.reconstruct(data, output_path)
        return JSONResponse(content={"status": "success", "pdf_url": f"/results/reconstructed_output.pdf"})
    except Exception as e:
        return JSONResponse(content={"error": str(e)}, status_code=500)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)